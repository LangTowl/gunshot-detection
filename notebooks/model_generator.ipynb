{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:47.124802Z",
     "start_time": "2025-01-19T16:37:47.121766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import io\n",
    "import librosa\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import Input\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "id": "c4fb003e35793f16",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:47.137274Z",
     "start_time": "2025-01-19T16:37:47.134811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths to data\n",
    "gunshot_path = \"../data/preprocessed/gunshot\"\n",
    "non_gunshot_path = \"../data/preprocessed/not-gunshot\"\n",
    "\n",
    "# Verify that paths exists\n",
    "print(os.path.exists(gunshot_path))\n",
    "print(os.path.exists(non_gunshot_path))"
   ],
   "id": "ab1497560f0f8d7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:47.162492Z",
     "start_time": "2025-01-19T16:37:47.160629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SAMPLE_RATE = 16000\n",
    "FRAME_SIZE = 2048\n",
    "HOP_SIZE = 128\n",
    "SEGMENT_LENGTH = SAMPLE_RATE * 2\n",
    "SEGMENT_HOP = SEGMENT_LENGTH // 2"
   ],
   "id": "5d075d7b1fa8abfc",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:47.177914Z",
     "start_time": "2025-01-19T16:37:47.174693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to generate numpy array of spectrogram from image\n",
    "def generate_spectrogram(wav_file_path, model_input_dims = (128, 128)):\n",
    "    # Load the .wav file\n",
    "    audio_segment, sr = librosa.load(wav_file_path, sr=SAMPLE_RATE)\n",
    "\n",
    "    # Compress channels\n",
    "    if audio_segment.ndim > 1:\n",
    "        audio_segment = audio_segment.squeeze()\n",
    "\n",
    "    # Compute the STFT\n",
    "    stft = librosa.stft(audio_segment, n_fft = FRAME_SIZE, hop_length = HOP_SIZE)\n",
    "    stft_magnitude = np.abs(stft) ** 2\n",
    "\n",
    "    # Convert to log-amplitude scale\n",
    "    stft_db = librosa.amplitude_to_db(stft_magnitude)\n",
    "\n",
    "    # Create the spectrogram plot\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.axis('off')\n",
    "    librosa.display.specshow(\n",
    "        stft_db,\n",
    "        sr=SAMPLE_RATE,\n",
    "        hop_length=HOP_SIZE,\n",
    "        x_axis='time',\n",
    "        y_axis='log',\n",
    "        cmap='magma'\n",
    "    )\n",
    "\n",
    "    # Save the figure into a memory buffer\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "    buf.seek(0)\n",
    "\n",
    "    # Load the image from the buffer and convert it to RGB\n",
    "    spectrogram_image = Image.open(buf).convert(\"RGB\")\n",
    "    spectrogram_image = spectrogram_image.resize((128, 128))  # Resize to (128, 128)\n",
    "\n",
    "    # Convert to numpy array and normalize the pixel values\n",
    "    spectrogram_image = np.array(spectrogram_image) / 255.0\n",
    "\n",
    "    # Close the plot to release memory\n",
    "    plt.close()\n",
    "\n",
    "    return np.expand_dims(spectrogram_image, axis = 0)"
   ],
   "id": "79440a0f68dd486f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:47.190767Z",
     "start_time": "2025-01-19T16:37:47.188215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to load and process images -> 0 for non-gunshot and 1 for gunshot\n",
    "def load_images(path, label):\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "\n",
    "        # Only accept images\n",
    "        valid_extensions = ['.wav']\n",
    "        if not any(filename.lower().endswith(ext) for ext in valid_extensions):\n",
    "            continue\n",
    "\n",
    "        # Load local path to image\n",
    "        img_path = os.path.join(path, filename)\n",
    "\n",
    "        print(f\"Attempting to load image {img_path}\")\n",
    "\n",
    "        try:\n",
    "            # Compute spectrogram\n",
    "            img = generate_spectrogram(img_path)\n",
    "\n",
    "            # Append image and corresponding label\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "        except Exception as error:\n",
    "            print(error)\n",
    "            continue\n",
    "\n",
    "    return images, labels"
   ],
   "id": "e19c15c4a44b711a",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:47.807636Z",
     "start_time": "2025-01-19T16:37:47.198216Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load gunshots\n",
    "gunshot_images, gunshot_labels = load_images(gunshot_path, 1)\n",
    "non_gunshot_images, non_gunshot_labels = load_images(non_gunshot_path, 0)"
   ],
   "id": "ee4908874d90b558",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load image ../data/preprocessed/gunshot/gunshot_0.wav\n",
      "Attempting to load image ../data/preprocessed/not-gunshot/not_gunshot_0_1.wav\n",
      "Attempting to load image ../data/preprocessed/not-gunshot/not_gunshot_0_0.wav\n",
      "Attempting to load image ../data/preprocessed/not-gunshot/not_gunshot_0_2.wav\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:47.817889Z",
     "start_time": "2025-01-19T16:37:47.815693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Combine gunshots and labels of both types\n",
    "images = np.array(gunshot_images + non_gunshot_images)\n",
    "labels = np.array(gunshot_labels + non_gunshot_labels)"
   ],
   "id": "571ba21aad9fc3aa",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:47.828260Z",
     "start_time": "2025-01-19T16:37:47.824968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 20% for testing 80# for training\n",
    "# X_train -> training subset for input features (images)\n",
    "# X_val   -> validation subset for input features (images)\n",
    "# y_train -> training subset for labels\n",
    "# y_val   -> validation subset for labels\n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)"
   ],
   "id": "736f1c8fbb48ef65",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:47.836756Z",
     "start_time": "2025-01-19T16:37:47.835118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Flatten the list into the required 4D shape (num_samples, 128, 128, 3)\n",
    "X_train = np.vstack(X_train)\n",
    "X_val = np.vstack(X_val)"
   ],
   "id": "1448dfc8d830fa0",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:47.887854Z",
     "start_time": "2025-01-19T16:37:47.843814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the CNN model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Input layer to define the input shape\n",
    "# Output -> 3D Tensor shape (128, 128, 3) -> RGB image 128x128 pixels\n",
    "model.add(Input(shape=(128, 128, 3)))\n",
    "\n",
    "# Applies 32 convolution filters of dimension 3x3 to input\n",
    "# Output -> (126, 126, 32)\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "# Reduces spatial dimensions by taking max value within overlapping 2x2 regions\n",
    "# Output -> (63, 63, 32)\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Applies 64 convolution filters of dimension 3x3 to input\n",
    "# Output -> (61, 61, 64)\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Reduces spatial dimensions by taking max value within overlapping 2x2 regions\n",
    "# Output -> (30, 30, 64)\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Applies 128 convolution filters of dimension 3x3 to input\n",
    "# Output -> (28, 28, 128)\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "# Reduces spatial dimensions by taking max value within overlapping 2x2 regions\n",
    "# Output -> (14, 14, 128)\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Converts 3D output from previous layer to 1D vector\n",
    "# Output -> (25088, )\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Applies 128 neurons to learn 'high level' features\n",
    "# Output -> (128, )\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "# Sigmoid function to determine gunshot or non-gunshot\n",
    "# 0 = non-gunshot, 1 = gunshot\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n"
   ],
   "id": "fcb00f2a2cd4c468",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:47.901015Z",
     "start_time": "2025-01-19T16:37:47.896373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compile the model:\n",
    "# Optimizer -> adam\n",
    "# loss function -> binary_crossentropy\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "id": "b69e88f947ee5e7a",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:59.645918Z",
     "start_time": "2025-01-19T16:37:59.052053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the model:\n",
    "# Epochs -> Total passes through model\n",
    "# Batch size -> # of samples the model processes before updating weights\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))"
   ],
   "id": "8bdd47e906779cbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 69ms/step - accuracy: 1.0000 - loss: 4.2756e-19 - val_accuracy: 1.0000 - val_loss: 1.4132e-23\n",
      "Epoch 2/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 55ms/step - accuracy: 1.0000 - loss: 2.3304e-21 - val_accuracy: 1.0000 - val_loss: 3.9004e-26\n",
      "Epoch 3/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 57ms/step - accuracy: 1.0000 - loss: 1.4384e-23 - val_accuracy: 1.0000 - val_loss: 1.2763e-28\n",
      "Epoch 4/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 53ms/step - accuracy: 1.0000 - loss: 1.0413e-25 - val_accuracy: 1.0000 - val_loss: 5.1171e-31\n",
      "Epoch 5/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 55ms/step - accuracy: 1.0000 - loss: 9.0914e-28 - val_accuracy: 1.0000 - val_loss: 2.5745e-33\n",
      "Epoch 6/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 55ms/step - accuracy: 1.0000 - loss: 9.7348e-30 - val_accuracy: 1.0000 - val_loss: 1.6552e-35\n",
      "Epoch 7/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 54ms/step - accuracy: 1.0000 - loss: 1.2959e-31 - val_accuracy: 1.0000 - val_loss: 1.3717e-37\n",
      "Epoch 8/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 53ms/step - accuracy: 1.0000 - loss: 2.1563e-33 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 54ms/step - accuracy: 1.0000 - loss: 4.5083e-35 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 53ms/step - accuracy: 1.0000 - loss: 1.1812e-36 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:38:02.849716Z",
     "start_time": "2025-01-19T16:38:02.807314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Determine performance metrics\n",
    "test_loss, test_acc = model.evaluate(X_val, y_val)\n",
    "print(f\"Test accuracy: {test_acc}\")"
   ],
   "id": "12874eda0d3e55e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 19ms/step - accuracy: 1.0000 - loss: 0.0000e+00\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T16:37:49.274633Z",
     "start_time": "2025-01-19T16:37:49.226933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save model\n",
    "model_name = \"test_model\"\n",
    "model.save(f'../models/{model_name}.h5')"
   ],
   "id": "ce72b190f1ccce25",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
